{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fa4e7-de05-4fe0-9690-7f4c339bac26",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Prompt the user for their Hugging Face token\n",
    "huggingface_token = input(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=huggingface_token)\n",
    "\n",
    "print(\"Successfully logged in to Hugging Face!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aef6be7-d605-4e77-ab2e-401b27a5e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "# for i in range(torch.cuda.device_count()):     \n",
    "#     print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b1cfd43-cd84-44bc-9268-09419b20599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample record from the dataset:\n",
      "{'role_1': 'Physicist_RoleType.ASSISTANT', 'topic;': 'Quantum mechanics', 'sub_topic': 'The Schrödinger equation and its solutions', 'message_1': 'What is the probability of finding a particle with a given energy in a one-dimensional infinite square well potential when the potential width is 2 nm and the particle has a mass of 5x10^-26 kg? Use the Schrödinger equation to solve for the allowed energy states and their wave functions.', 'message_2': 'To find the probability of finding a particle with a given energy in a one-dimensional infinite square well potential, we first need to solve the Schrödinger equation for the allowed energy states and their wave functions.\\n\\nThe time-independent Schrödinger equation for a one-dimensional infinite square well potential is given by:\\n\\n- (ħ^2 / 2m) * (d^2ψ(x) / dx^2) = E * ψ(x)\\n\\nwhere ħ is the reduced Planck constant (1.0545718 × 10^-34 Js), m is the mass of the particle (5 × 10^-26 kg), E is the energy of the particle, and ψ(x) is the wave function.\\n\\nThe boundary conditions for the infinite square well potential are:\\n\\nψ(0) = 0 and ψ(a) = 0\\n\\nwhere a is the width of the well (2 nm = 2 × 10^-9 m).\\n\\nThe general solution to the Schrödinger equation is:\\n\\nψ(x) = A * sin(kx) + B * cos(kx)\\n\\nwhere A and B are constants, and k = sqrt(2mE) / ħ.\\n\\nApplying the boundary conditions:\\n\\nψ(0) = A * sin(0) + B * cos(0) = B = 0\\nψ(a) = A * sin(ka) = 0\\n\\nSince A cannot be zero (otherwise the wave function would be trivial), sin(ka) must be zero. This occurs when ka = nπ, where n is an integer (1, 2, 3, ...). Therefore, the allowed wave numbers are:\\n\\nk_n = nπ / a\\n\\nThe allowed energy states are given by:\\n\\nE_n = (ħ^2 * k_n^2) / (2m) = (ħ^2 * n^2 * π^2) / (2m * a^2)\\n\\nThe corresponding wave functions are:\\n\\nψ_n(x) = A_n * sin(k_n * x)\\n\\nwhere A_n is a normalization constant.\\n\\nTo find the probability of finding a particle with a given energy E, we need to know the initial state of the particle. If the particle is in the nth energy state, the probability of finding it with energy E_n is 1. If the particle is in a superposition of energy states, we need to find the coefficients of the superposition and calculate the probability accordingly.\\n\\nIn summary, the allowed energy states and their wave functions for a particle in a one-dimensional infinite square well potential can be found using the Schrödinger equation. The probability of finding a particle with a given energy depends on the initial state of the particle and requires additional information.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "dataset_name = \"akhilfau/physics_decontaminated_2\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")  # Adjust the split if needed (e.g., \"test\" or \"validation\")\n",
    "\n",
    "# Print a sample record\n",
    "print(\"Sample record from the dataset:\")\n",
    "print(dataset[0])  # Prints the first record from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "340e93b1-ff80-414c-a21c-8beb6fff9eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,638,400 || all params: 363,459,520 || trainable%: 0.4508\n",
      "length of the data:  20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/avallala2023/anaconda3/envs/Thesis/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 2:18:55, Epoch 11/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.199300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.197500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avallala2023/anaconda3/envs/Thesis/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/avallala2023/anaconda3/envs/Thesis/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/avallala2023/anaconda3/envs/Thesis/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.2113407958984375, metrics={'train_runtime': 8338.1042, 'train_samples_per_second': 28.784, 'train_steps_per_second': 0.45, 'total_flos': 2.328401992482816e+17, 'train_loss': 0.2113407958984375, 'epoch': 11.9824})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "dataset = load_dataset(\"akhilfau/physics_decontaminated_2\", split=\"train\")\n",
    "\n",
    "# Step 2: Load the Pretrained Model and Tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define the padding token if not already set\n",
    "tokenizer.pad_token = tokenizer.eos_token or tokenizer.bos_token or \"[PAD]\"\n",
    "\n",
    "# Step 3: Configure LoRA with PEFT\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for causal language modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters to confirm LoRA is applied\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Step 4: Preprocess the Dataset\n",
    "def preprocess_function(examples):\n",
    "    # Concatenate the problem and solution for causal LM\n",
    "    inputs = [f\"Problem: {problem}\\nSolution: {solution}\" for problem, solution in zip(examples[\"message_1\"], examples[\"message_2\"])]\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    # Labels are the same as input_ids for causal LM\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Split into train and validation sets\n",
    "# train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "# train_dataset = train_test_split[\"train\"]\n",
    "# eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(\"length of the data: \",tokenized_dataset.shape[0])\n",
    "\n",
    "# Step 5: Define Training Arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     logging_steps=100,\n",
    "#     save_steps=500,\n",
    "#     learning_rate=5e-4,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=2,\n",
    "#     push_to_hub=False,  # Set to True if you want to push to Hugging Face Hub\n",
    "#     logging_dir=\"./logs\",\n",
    "#     bf16=False,  # Disable BFloat16\n",
    "#     fp16=False,  # Disable FP16\n",
    "# )\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     logging_steps=100,\n",
    "#     save_steps=500,\n",
    "#     learning_rate=5e-4,  # Reduce the learning rate\n",
    "#     lr_scheduler_type=\"cosine\",  # Use a more adaptive scheduler\n",
    "#     per_device_train_batch_size=4,  # Increase if memory allows\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     num_train_epochs=8,  # Train for more epochs\n",
    "#     weight_decay=0.1,  # Regularization\n",
    "#     save_total_limit=2,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     push_to_hub=False,\n",
    "#     #gradient_checkpointing=True,  # Reduce memory usage if needed\n",
    "#     bf16=False,  # Disable BFloat16\n",
    "#     fp16=False,  # Disable FP16\n",
    "# )\n",
    "\n",
    "#Step5:\n",
    "# Estimate total training steps\n",
    "use_bf16 = torch.cuda.is_bf16_supported()\n",
    "dataset_size = tokenized_dataset.shape[0] # Adjust based on dataset size\n",
    "batch_size = 4  # Adjust based on available GPU memory\n",
    "grad_accum_steps = 4  # Simulate larger batch without using more memory\n",
    "total_steps = (dataset_size // (batch_size * grad_accum_steps)) * 3  # 3 epochs\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum_steps,\n",
    "    warmup_steps=100,  # More warmup steps for stability\n",
    "    max_steps=total_steps,  # Train across full dataset\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not use_bf16,  # Use FP16 if bf16 is not available\n",
    "    bf16=use_bf16,  # Use BF16 if supported\n",
    "    logging_steps=100,  # Reduce logging frequency for better efficiency\n",
    "    save_steps=1000,  # Save every 1000 steps\n",
    "    save_total_limit=2,  # Keep last 2 checkpoints\n",
    "    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"wandb\",  # Log to W&B\n",
    ")\n",
    "# Step 6: Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    "    # eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Step 7: Train the Model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d54499-fd63-4c9e-9078-bfecd7c6e6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2/tokenizer_config.json',\n",
       " './fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2/special_tokens_map.json',\n",
       " './fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2/vocab.json',\n",
       " './fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2/merges.txt',\n",
       " './fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2/added_tokens.json',\n",
       " './fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Save the Model and Tokenizer\n",
    "save_dir = \"./fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2\"\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ab19d60-2a4b-4647-bdb6-f0f8c4aa2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a077f1b-d9ef-4d82-b6a3-087e3b69d110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 66, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Generate a response\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.10/site-packages/transformers/generation/utils.py:2079\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   2077\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2079\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[1;32m   2085\u001b[0m max_cache_length \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.10/site-packages/transformers/generation/utils.py:1416\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1415\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1420\u001b[0m     )\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 66, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the path to the locally saved model\n",
    "local_model_path = \"./fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "\n",
    "# Test with a sample input\n",
    "#input_text = \"What is the Schrödinger equation?\"\n",
    "input_text = \"A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car?Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4 m/s²\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90693079-4c95-45dc-afc2-cd097778cfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2', endpoint='https://huggingface.co', repo_type='model', repo_id='akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder, login\n",
    "# Create the repository (if it doesn't exist)\n",
    "repo_name=\"akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2\"\n",
    "create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02c6e6f4-12d1-4cc2-9369-c2144e73bf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6deb38c9299b4168a2d00eb34bde2427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80d4d416d87486db7d220ffcbf65889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9a45a19e684eb9ac5a552100b7797c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39511f2f23d4f26ad7724acc5678b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2/commit/dd94f79c05c8ae171de188cd8a802650396062b1', commit_message='Upload folder using huggingface_hub', commit_description='', oid='dd94f79c05c8ae171de188cd8a802650396062b1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2', endpoint='https://huggingface.co', repo_type='model', repo_id='akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Push to Hugging Face Hub\n",
    "trainer.push_to_hub(commit_message=\"Fine-tuned smolLM2-360M with LoRA on camel-ai/physics_2\")\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"./fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2\",\n",
    "    repo_id=\"akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94116745-9bb6-4da6-80f5-c488e35e4810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car?Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4 m/s²\n",
      "\n",
      "asked by Anonymous on March 1, 2018\n",
      "10. ## Physics\n",
      "\n",
      "A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car? Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4 m/s²\n",
      "\n",
      "asked by Anonymous on March 1, 2018\n",
      "11. ## Physics\n",
      "\n",
      "A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car? Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4 m/s²\n",
      "\n",
      "asked by Anonymous on March 1, 2018\n",
      "12. ## Physics\n",
      "\n",
      "A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car? Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4 m/s²\n",
      "\n",
      "asked by Anonymous on March 1, 2018\n",
      "13. ## Physics\n",
      "\n",
      "A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car? Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4 m/s²\n",
      "\n",
      "asked by Anonymous on March 1, 2018\n",
      "14. ## Physics\n",
      "\n",
      "A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car? Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the Hugging Face model path (update with the correct repository path)\n",
    "model_path = \"akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics_2\"\n",
    "\n",
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Test with a sample input\n",
    "#input_text = \"What is the Schrödinger equation?\"\n",
    "input_text = \"A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car?Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4 m/s²\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(**inputs, max_length=512)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6410cd-c291-4aae-8e79-84bc11cab60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car?Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4 m/s²E) 6 m/s²\n",
      "\n",
      "ANSWER: A) 3 m/s²\n",
      "\n",
      "A car accelerates from rest to a speed of 30 m/s in 10 seconds. To find the acceleration, we need to determine the distance covered and the change in speed.\n",
      "\n",
      "Distance (d) = total speed × time\n",
      "\n",
      "Change in speed (Δs) = final speed - initial speed\n",
      "\n",
      "First, we need to find the final speed (v) of the car:\n",
      "\n",
      "v = final speed = 30 m/s\n",
      "\n",
      "Now, we can find the change in distance (Δd) by adding the initial distance (d) to the final distance (v):\n",
      "\n",
      "Δd = d + v = 0 + 30 m/s = 30 m/s\n",
      "\n",
      "Now we can find the change in time (Δt) by dividing the change in distance by the time it took to accelerate from rest to 30 m/s:\n",
      "\n",
      "Δt = Δd / Δs = 30 m/s / (30 m/s) = 1 second\n",
      "\n",
      "Now we can find the acceleration (a) using the formula:\n",
      "\n",
      "a = Δt / Δs\n",
      "\n",
      "a = 1 s / 30 m = 1/30 m/s²\n",
      "\n",
      "So, the acceleration of the car is 1/30 m/s² or 3/30 m/s² (1/10 m/s² or 0.3 m/s²).\n",
      "\n",
      "Note: All options have the same acceleration (a = 3 m/s²), so the correct answer is Option A. However, it's important to note that the question asks for the acceleration per second, which would be 3 m/s² × 10 seconds = 30 m/s². Since the question asks for the acceleration per second, we should only consider the change in speed (Δs = 30 m/s) and not the change in time (Δt = 1 second).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the Hugging Face model path (update with the correct repository path)\n",
    "model_path = \"akhilfau/Instruction_fine_tuned_on_camel_ai_physics\"\n",
    "\n",
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Test with a sample input\n",
    "#input_text = \"What is the Schrödinger equation?\"\n",
    "input_text = \"A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car?Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4 m/s²\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(**inputs, max_length=512)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d81931-84e9-455c-aa3f-71349cea2414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ball is thrown vertically upward with an initial velocity of 20 m/s. Ignoring air resistance, how long will it take for the ball to reach its highest point? Assume the Earth's gravitational acceleration is 9.81 m/s^2. Calculate the time duration of the thrown ball's motion using the formula for vertical motion-\n",
      "\n",
      "t = v*sin(θ) / g\n",
      "\n",
      "where t is the time duration, v is the initial velocity, θ is the angle of projection, and g is the acceleration due to gravity.\n",
      "\n",
      "Given:\n",
      "v = 20 m/s\n",
      "θ = 0° (thrown directly upwards)\n",
      "g = 9.81 m/s² (Earth's gravitational acceleration)\n",
      "\n",
      "First, we need to find the angle of projection (θ) using the initial velocity and distance traveled (x). We can use the formula:\n",
      "\n",
      "x = v*sin(θ)\n",
      "\n",
      "x = 20 m/s * sin(0°)\n",
      "x = 20 m/s * 0 m/s\n",
      "x = 0 m\n",
      "\n",
      "Since x = 0, we cannot have a sine value of 0. This means that the thrown ball will not reach its highest point and will continue to fall until it reaches the ground it was thrown from. Therefore, the time duration of the thrown ball's motion is infinite.\n",
      "\n",
      "However, the problem statement does not provide the distance traveled by the ball (x). To find the time duration, we would need this information. In the meantime, the given time is too long to be meaningful for the given circumstances. Throwing a ball directly upwards will result in a time duration of zero for the thrown ball's motion. Any shorter time duration would indicate a faster fall rate, which is not the case here. The problem statement should have provided the distance traveled by the ball or a finite value to compare against.\n",
      "\n",
      "After considering the given information, the problem statement cannot be solved with the provided data. The thrown ball will remain on the ground indefinitely due to the provided time duration. Please provide the distance traveled by the ball (x) to proceed with the problem.\n",
      "\n",
      "Please provide the distance traveled by the ball (x) to proceed with the problem. Once you have the distance traveled, we can calculate the time duration using the formula for vertical motion.\n",
      "\n",
      "Assuming the distance traveled by the ball is provided, we can calculate the time\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the Hugging Face model path (update with the correct repository path)\n",
    "model_path = \"akhilfau/Instruction_fine_tuned_on_camel_ai_physics\"\n",
    "\n",
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Test with a sample input\n",
    "#input_text = \"What is the Schrödinger equation?\"\n",
    "input_text = \"A ball is thrown vertically upward with an initial velocity of 20 m/s. Ignoring air resistance, how long will it take for the ball to reach its highest point?\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(**inputs, max_length=512)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75481075-4775-4c56-9e53-85ae062ee537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 5 kg box is being pulled across a rough surface with a force of 40 N. If the coefficient of kinetic friction between the box and the surface is 0.3, what is the acceleration of the box?\n",
      "\n",
      "(A) 0 m/s² (B) 3 m/s² (C) 0.3 m/s² (D) 9 m/s² (E) Not sure\n",
      "\n",
      "(F) Please help solve this physics problem for a physics project – If you have the answer, I would love to see it! If not, don’t worry about the answer itself, just want to know how you solved it. Thanks in advance for your time and help.\n",
      "\n",
      "1. A. (F)\n",
      "\n",
      "(F) = 40 N\n",
      "\n",
      "(F) = 0.3 * 40 N\n",
      "\n",
      "(F) = 12 N\n",
      "\n",
      "(F) = 0.3 * 12 N\n",
      "\n",
      "(F) = 3 N\n",
      "\n",
      "(F) = 0.3 * 3 N\n",
      "\n",
      "(F) = 9 N\n",
      "\n",
      "(F) = 9 m/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = 9 cm/s²\n",
      "\n",
      "(F) = \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the Hugging Face model path (update with the correct repository path)\n",
    "model_path = \"akhilfau/Instruction_fine_tuned_on_camel_ai_physics\"\n",
    "\n",
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Test with a sample input\n",
    "#input_text = \"What is the Schrödinger equation?\"\n",
    "input_text = \"A 5 kg box is being pulled across a rough surface with a force of 40 N. If the coefficient of kinetic friction between the box and the surface is 0.3, what is the acceleration of the box?\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(**inputs, max_length=512)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9030edd0-035c-4b91-8c40-9bf98b887e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3a7e1b9f3446e9a1be7489cb8a82d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343629d495684d20b4febf1c0ef7dad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ab9fb1d0df4359a90672713aca1689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f565cb950449eeaac2bc719f4b6ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1aab32bfa7474fa640f50f05a1d3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cdd8a070b74b8bb03e46f5e1995861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7d3e0d44314f23916bbde41cde9919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c625aed71b48cdbe567058b26a439f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 5 kg box is being pulled across a rough surface with a force of 40 N. If the coefficient of kinetic friction between the box and the surface is 0.3, what is the acceleration of the box?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the Hugging Face model path (update with the correct repository path)\n",
    "model_path = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Test with a sample input\n",
    "#input_text = \"What is the Schrödinger equation?\"\n",
    "input_text = \"A 5 kg box is being pulled across a rough surface with a force of 40 N. If the coefficient of kinetic friction between the box and the surface is 0.3, what is the acceleration of the box?\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(**inputs, max_length=512)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7be750d-b825-436e-9982-4ea093454906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Clear cache\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Reset memory allocations and free up GPU memory\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3eb5046-4c83-46fa-b037-42505c35794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: lighteval: command not found\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    " \n",
    "!lighteval accelerate \\\n",
    "    --model_args \"pretrained=$MODEL\" \\\n",
    "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
    "    --override_batch_size 16 \\\n",
    "    --output_dir $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65621902-b452-4a04-9333-9ae109fcaaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics\"\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    " \n",
    "!lighteval accelerate \\\n",
    "    --model_args \"pretrained=$MODEL\" \\\n",
    "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
    "    --override_batch_size 16 \\\n",
    "    --output_dir $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1c570-fcac-4df2-8b09-42d24e9d7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    "!lighteval accelerate \\\n",
    "    --model_args \"pretrained=$MODEL\" \\\n",
    "    --tasks \"leaderboard|mmlu:conceptual_physics|0|0\" \\\n",
    "    --override_batch_size 16 \\\n",
    "    --output_dir \"$OUTPUT_DIR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2131f-d03b-4150-8d52-f5a00f5d2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics\"\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    "!lighteval accelerate \\\n",
    "    --model_args \"pretrained=$MODEL\" \\\n",
    "    --tasks \"leaderboard|mmlu:conceptual_physics|0|0\" \\\n",
    "    --override_batch_size 16 \\\n",
    "    --output_dir \"$OUTPUT_DIR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32d7ac-b29b-46b0-b642-1fa29f7fae95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f389c-5cb0-48ab-8a4b-11253ee684a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc48432-e486-463b-b5d5-d9128bf8dc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aafe5cd-e56c-4607-ae9b-d01f37909a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
