{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fa4e7-de05-4fe0-9690-7f4c339bac26",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Prompt the user for their Hugging Face token\n",
    "huggingface_token = input(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=huggingface_token)\n",
    "\n",
    "print(\"Successfully logged in to Hugging Face!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aef6be7-d605-4e77-ab2e-401b27a5e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "# for i in range(torch.cuda.device_count()):     \n",
    "#     print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b1cfd43-cd84-44bc-9268-09419b20599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample record from the dataset:\n",
      "{'Question_number': 1.1, 'Questions': '\\\\documentclass{article}\\n\\\\usepackage{amsmath}\\n\\n\\\\begin{document}\\n\\nA car starts from rest at a constant acceleration of \\\\(2.0 \\\\, \\\\text{m/s}^2\\\\). At the same instant, a truck traveling with a constant speed of \\\\(10 \\\\, \\\\text{m/s}\\\\) overtakes and passes the car.\\n\\n\\\\begin{enumerate}\\n    \\\\item[(a)] How far beyond the starting point will the car overtake the truck?\\n    \\\\item[(b)] After what time will this happen?\\n    \\\\item[(c)] At that instant, what will be the speed of the car?\\n\\\\end{enumerate}\\n\\n\\\\end{document}', 'Sub-category': 'Motion in One Dimension', 'Category': 'Kinematics and statics', 'Solutions': '\\\\documentclass{article}\\n\\\\usepackage{amsmath}\\n\\n\\\\begin{document}\\n\\n\\\\section*{Physics Problem: Motion of Truck and Car}\\n\\n\\\\subsection*{(a) Equations of Motion}\\n\\nThe equation of motion for the truck is given by:\\n\\\\begin{equation}\\n    s = ut \\\\tag{1}\\n\\\\end{equation}\\n\\nThe equation of motion for the car is given by:\\n\\\\begin{equation}\\n    s = \\\\frac{1}{2} a t^2 \\\\tag{2}\\n\\\\end{equation}\\n\\nThe graphs for equations (1) and (2) are shown in Fig. 1.13. By eliminating \\\\( t \\\\) between the two equations, we have:\\n\\\\begin{equation}\\n    s \\\\left( 1 - \\\\frac{1}{2} \\\\frac{as}{u^2} \\\\right) = 0 \\\\tag{3}\\n\\\\end{equation}\\n\\n\\\\textbf{Fig. 1.13} shows the graphical representation of these equations. The solutions are either \\\\( s = 0 \\\\) or \\\\( 1 - \\\\frac{1}{2} \\\\frac{as}{u^2} = 0 \\\\).\\n\\nThe first solution corresponds to the result that the truck overtakes the car at \\\\( s = 0 \\\\) and therefore at \\\\( t = 0 \\\\).\\n\\nThe second solution gives:\\n\\\\[\\ns = \\\\frac{2u^2}{a} = \\\\frac{2 \\\\times 10^2}{2} = 100 \\\\, \\\\text{m}\\n\\\\]\\n\\n\\\\subsection*{(b) Time Calculation}\\n\\nThe time \\\\( t \\\\) when the truck overtakes the car is given by:\\n\\\\[\\nt = \\\\frac{s}{u} = \\\\frac{100}{10} = 10 \\\\, \\\\text{s}\\n\\\\]\\n\\n\\\\subsection*{(c) Final Velocity of the Car}\\n\\nThe final velocity \\\\( v \\\\) of the car is:\\n\\\\[\\nv = at = 2 \\\\times 10 = 20 \\\\, \\\\text{m/s}\\n\\\\]\\n\\n\\\\end{document}'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Load dataset from CSV\n",
    "dataset = load_dataset(\"csv\", data_files=\"SF_train.csv\",encoding=\"ISO-8859-1\",split=\"train\")\n",
    " \n",
    "# Check available splits\n",
    "#print(\"Available splits:\", dataset.keys())\n",
    " \n",
    "# Print a sample record from the training set\n",
    "print(\"Sample record from the dataset:\")\n",
    "print(dataset[0])  # Accessing first sample from \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340e93b1-ff80-414c-a21c-8beb6fff9eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,638,400 || all params: 363,459,520 || trainable%: 0.4508\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab01a1a554b4d5b9562a28b01241fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the data:  951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makhilvallala0115\u001b[0m (\u001b[33makhilvallala0115-florida-atlantic-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/avallala2023/Desktop/Thesis/Instruction Fine Tuning/wandb/run-20250311_194759-gimx3dge</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/akhilvallala0115-florida-atlantic-university/huggingface/runs/gimx3dge' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/akhilvallala0115-florida-atlantic-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/akhilvallala0115-florida-atlantic-university/huggingface' target=\"_blank\">https://wandb.ai/akhilvallala0115-florida-atlantic-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/akhilvallala0115-florida-atlantic-university/huggingface/runs/gimx3dge' target=\"_blank\">https://wandb.ai/akhilvallala0115-florida-atlantic-university/huggingface/runs/gimx3dge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avallala2023/anaconda3/envs/Thesis/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='177' max='177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [177/177 12:36, Epoch 11/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.998600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=177, training_loss=0.6189043239011602, metrics={'train_runtime': 763.5005, 'train_samples_per_second': 14.837, 'train_steps_per_second': 0.232, 'total_flos': 2.18200243470336e+16, 'train_loss': 0.6189043239011602, 'epoch': 11.8})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "#dataset = load_dataset(\"akhilfau/physics_decontaminated_2\", split=\"train\")\n",
    "\n",
    "# Load dataset from CSV\n",
    "dataset = load_dataset(\"csv\", data_files=\"SF_train.csv\",encoding=\"ISO-8859-1\",split=\"train\")\n",
    "\n",
    "# Step 2: Load the Pretrained Model and Tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define the padding token if not already set\n",
    "tokenizer.pad_token = tokenizer.eos_token or tokenizer.bos_token or \"[PAD]\"\n",
    "\n",
    "# Step 3: Configure LoRA with PEFT\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for causal language modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters to confirm LoRA is applied\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Step 4: Preprocess the Dataset\n",
    "def preprocess_function(examples):\n",
    "    # Concatenate the problem and solution for causal LM\n",
    "    inputs = [f\"Problem: {problem}\\nSolution: {solution}\" for problem, solution in zip(examples[\"Questions\"], examples[\"Solutions\"])]\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    #model_inputs = tokenizer(inputs, truncation=True, padding=True)\n",
    "    \n",
    "    # Labels are the same as input_ids for causal LM\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"length of the data: \",tokenized_dataset.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "#Step5:\n",
    "# Estimate total training steps\n",
    "use_bf16 = torch.cuda.is_bf16_supported()\n",
    "dataset_size = tokenized_dataset.shape[0] # Adjust based on dataset size\n",
    "batch_size = 4  # Adjust based on available GPU memory\n",
    "grad_accum_steps = 4  # Simulate larger batch without using more memory\n",
    "total_steps = (dataset_size // (batch_size * grad_accum_steps)) * 3  # 3 epochs\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum_steps,\n",
    "    warmup_steps=100,  # More warmup steps for stability\n",
    "    max_steps=total_steps,  # Train across full dataset\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not use_bf16,  # Use FP16 if bf16 is not available\n",
    "    bf16=use_bf16,  # Use BF16 if supported\n",
    "    logging_steps=100,  # Reduce logging frequency for better efficiency\n",
    "    save_steps=1000,  # Save every 1000 steps\n",
    "    save_total_limit=2,  # Keep last 2 checkpoints\n",
    "    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"wandb\",  # Log to W&B\n",
    ")\n",
    "# Step 6: Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    "    # eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Step 7: Train the Model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d54499-fd63-4c9e-9078-bfecd7c6e6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2/tokenizer_config.json',\n",
       " './fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2/special_tokens_map.json',\n",
       " './fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2/vocab.json',\n",
       " './fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2/merges.txt',\n",
       " './fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2/added_tokens.json',\n",
       " './fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Save the Model and Tokenizer\n",
    "save_dir = \"./fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2\"\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab19d60-2a4b-4647-bdb6-f0f8c4aa2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a077f1b-d9ef-4d82-b6a3-087e3b69d110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car?Options:A) 3 m/s²B)2.5 m/s²C) 5 m/s²D) 4 m/s²\n",
      "\n",
      "### A 1000 kg car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car? Options:A) 3 m/s²B)2.5 m/s²C) 5 m/s²D) 4 m/s²\n",
      "\n",
      "A 1000 kg car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car? Options:A) 3 m/s²B)2.5 m/s²C) 5 m/s²D) 4 m/s²...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the path to the locally saved model\n",
    "local_model_path = \"./fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "\n",
    "# Test with a sample input\n",
    "input_text = \"A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car?Options:A) 3 m/s²B)2.5 m/s²C) 5 m/s²D) 4 m/s²\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(**inputs,max_length=512)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90693079-4c95-45dc-afc2-cd097778cfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2', endpoint='https://huggingface.co', repo_type='model', repo_id='akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder, login\n",
    "# Create the repository (if it doesn't exist)\n",
    "repo_name=\"akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2\"\n",
    "create_repo(repo_id=repo_name, repo_type=\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02c6e6f4-12d1-4cc2-9369-c2144e73bf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c39fb43da84d99bc0d1fb0ea0f1951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbcc5619150475a9b78d8d6a174b3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b53b3997984b0d8b8da35d8a66745b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a4411486ad41b2a3b00f7c3ce4d00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2/commit/9df07b5c62628bbf980e36ba66883376bf8733ec', commit_message='Upload folder using huggingface_hub', commit_description='', oid='9df07b5c62628bbf980e36ba66883376bf8733ec', pr_url=None, repo_url=RepoUrl('https://huggingface.co/akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2', endpoint='https://huggingface.co', repo_type='model', repo_id='akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Push to Hugging Face Hub\n",
    "trainer.push_to_hub(commit_message=\"Fine-tuned smolLM2-360M with LoRA on text-book-physics_2\")\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"./fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2\",\n",
    "    repo_id=\"akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94116745-9bb6-4da6-80f5-c488e35e4810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 66, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Generate a response\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# output = model.generate(**inputs,max_length=50)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(tokenizer.decode(output[0], skip_special_tokens=True))\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add attention mask\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.10/site-packages/transformers/generation/utils.py:2079\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   2077\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2079\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[1;32m   2085\u001b[0m max_cache_length \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.10/site-packages/transformers/generation/utils.py:1416\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1415\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1417\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1420\u001b[0m     )\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 66, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the Hugging Face model path (update with the correct repository path)\n",
    "model_path = \"akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-text-book-physics_2\"\n",
    "\n",
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Test with a sample input\n",
    "input_text = \"A car accelerates uniformly from rest to a speed of 30 m/s in 10 seconds. What is the acceleration of the car?Options:A) 3 m/s²B) 2.5 m/s²C) 5 m/s²D) 4 m/s²\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "# output = model.generate(**inputs,max_length=50)\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Add attention mask\n",
    "    max_length=50\n",
    ")\n",
    " \n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7be750d-b825-436e-9982-4ea093454906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Clear cache\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Reset memory allocations and free up GPU memory\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3eb5046-4c83-46fa-b037-42505c35794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: lighteval: command not found\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    " \n",
    "!lighteval accelerate \\\n",
    "    --model_args \"pretrained=$MODEL\" \\\n",
    "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
    "    --override_batch_size 16 \\\n",
    "    --output_dir $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65621902-b452-4a04-9333-9ae109fcaaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: lighteval: command not found\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics\"\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    " \n",
    "!lighteval accelerate \\\n",
    "    --model_args \"pretrained=$MODEL\" \\\n",
    "    --tasks \"leaderboard|mmlu:college_physics|0|0\" \\\n",
    "    --override_batch_size 16 \\\n",
    "    --output_dir $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1c570-fcac-4df2-8b09-42d24e9d7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    "!lighteval accelerate \\\n",
    "    --model_args \"pretrained=$MODEL\" \\\n",
    "    --tasks \"leaderboard|mmlu:conceptual_physics|0|0\" \\\n",
    "    --override_batch_size 16 \\\n",
    "    --output_dir \"$OUTPUT_DIR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2131f-d03b-4150-8d52-f5a00f5d2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"akhilfau/fine-tuned-smolLM2-360M-with-LoRA-on-camel-ai-physics\"\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    "!lighteval accelerate \\\n",
    "    --model_args \"pretrained=$MODEL\" \\\n",
    "    --tasks \"leaderboard|mmlu:conceptual_physics|0|0\" \\\n",
    "    --override_batch_size 16 \\\n",
    "    --output_dir \"$OUTPUT_DIR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32d7ac-b29b-46b0-b642-1fa29f7fae95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f389c-5cb0-48ab-8a4b-11253ee684a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc48432-e486-463b-b5d5-d9128bf8dc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aafe5cd-e56c-4607-ae9b-d01f37909a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
